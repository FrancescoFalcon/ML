
# Training Configuration for Balatro RL
# =====================================

environment:
  max_ante: 8  # Training diretto ad ante 8 per strategia avanzata
  starting_money: 4
  deck_size: 52 # Not directly used by env but for info
  hand_size: 8 # Max cards in hand
  max_jokers: 5 # Max jokers player can have

training:
  algorithm: "PPO"
  total_timesteps: 25000  # Dimezzato da 50000 a 25000
  learning_rate: 0.00008
  batch_size: 256  # Ridotto per velocizzare
  n_steps: 1024 # Ridotto per feedback pi√π frequente
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.05
  vf_coef: 0.5
  max_grad_norm: 0.5
  n_envs: 4 # Ridotto per test veloce
  device: "cpu" # CPU per test veloce
  num_cpu_workers: 8

evaluation:
  eval_freq: 5000 # Dimezzato da 10000 a 5000 - Evaluate every N timesteps
  n_eval_episodes: 25 # Ridotto per velocizzare
  deterministic: true

logging:
  log_dir: "./data/logs/"
  model_dir: "./data/models/"
  use_wandb: true
  wandb_project: "balatro-rl"
  wandb_entity: null # Replace with your wandb entity if applicable
  save_freq: 25000 # Dimezzato da 50000 a 25000 - How often to save the model (for wandb)

curriculum:
  enabled: true  # Riabilitato per test completo del curriculum
  stages:
    - max_ante: 2
      timesteps: 25000  # Dimezzato da 50000 a 25000
    - max_ante: 3
      timesteps: 25000  # Dimezzato da 50000 a 25000
    - max_ante: 4
      timesteps: 25000  # Dimezzato da 50000 a 25000
    - max_ante: 5
      timesteps: 25000  # Dimezzato da 50000 a 25000
  n_eval_episodes: 10  # Ridotto per velocizzare
