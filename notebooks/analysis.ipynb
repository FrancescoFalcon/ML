
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balatro RL Analysis\n",
    "\n",
    "This notebook provides analysis and visualization of the Balatro RL training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from stable_baselines3 import PPO\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "from balatro_env import BalatroEnv\n",
    "from utils import load_results, BalatroMetrics, plot_training_progress, create_performance_report, analyze_hand_preferences\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # Updated style\n",
    "sns.set_palette('deep')\n",
    "\n",
    "print("Libraries loaded and paths configured.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Analyze Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load logs from Monitor wrapper (if using direct log files, not WandB)\n",
    "# You would typically use tensorboard or wandb UI for live monitoring.\n",
    "# For analysis after training, you might load saved evaluation results.\n",
    "\n",
    "# Assuming evaluation results are saved to 'data/results/evaluation_results.pkl'\n",
    "results_path = '../data/results/evaluation_results.pkl'\n",
    "metrics_obj = BalatroMetrics()\n",
    "\n",
    "try:\n",
    "    # In a real scenario, you'd populate BalatroMetrics from raw log data or a structured eval output\n",
    "    # For simplicity here, if evaluate_agent saves a dict, you'd convert it.\n",
    "    # Let's assume 'evaluate_agent' from training.py was run and its output saved.\n",
    "    # The evaluate_agent function already uses BalatroMetrics internally.\n",
    "    # If you want to load raw logs from monitor, you would use: \n",
    "    # from stable_baselines3.common.results_plotter import load_results as sb3_load_results\n",
    "    # log_dir = '../data/logs/' # Or the specific stage log dir\n",
    "    # agent_results = sb3_load_results(log_dir)\n",
    "    # print(agent_results.head())\n",
    "    \n",
    "    # For now, let's just create some dummy data or load a placeholder\n",
    "    # In practice, you'd save the metrics_tracker object directly if you wanted to reload it.\n",
    "    # As 'evaluate_agent' saves plots and reports, you'd look at those files.\n",
    "    \n",
    "    print("To analyze, ensure you have run 'python src/training.py --mode eval' first.\n")\n",
    "    print("Check 'evaluation_progress.png', 'evaluation_distributions.png', and 'evaluation_report.md' in your project root or data/plots.")\n",
    "    \n",
    "    # If you saved a raw BalatroMetrics object:\n",
    "    # metrics_obj = load_results(results_path) # Needs to be saved as pickle of BalatroMetrics object\n",
    "    # stats = metrics_obj.get_statistics()\n",
    "    # print("Loaded Evaluation Statistics:")\n",
    "    # for k, v in stats.items():\n",
    "    #     print(f"- {k}: {v}")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f"Warning: Results file not found at {results_path}. Run evaluation first.")\n",
    "    # Create dummy data for demonstration if file not found\n",
    "    metrics_obj.episode_rewards = np.random.rand(100) * 100 - 20 # Example rewards\n",
    "    metrics_obj.antes_reached = np.random.randint(1, 9, 100) # Example antes\n",
    "    metrics_obj.blinds_beaten = np.random.randint(0, 24, 100) # Example blinds\n",
    "    metrics_obj.scores_achieved = np.random.rand(100) * 1000 # Example scores\n",
    "    metrics_obj.hand_types_played = np.random.choice(['high_card', 'pair', 'flush'], 200).tolist()\n",
    "    metrics_obj.jokers_active_per_episode = [['joker'], ['jolly_joker'], ['greedy_joker']] * 33 # Example jokers\n",
    "    print("Using dummy data for analysis.")\n",
    "    \n",
    "stats = metrics_obj.get_statistics()\n",
    "print("\n--- Overall Statistics ---")\n",
    "for k, v in stats.items():\n",
    "    print(f"- {k}: {v}")\n",
    "\n",
    "if metrics_obj.episode_rewards:\n",
    "    plot_training_progress(metrics_obj.episode_rewards, metrics_obj.antes_reached, metrics_obj.blinds_beaten, save_path=None) # Display in notebook\n",
    "    metrics_obj.plot_distributions(save_path=None) # Display in notebook\n",
    "    analyze_hand_preferences(metrics_obj.hand_types_played, save_path=None) # Display in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model (ensure it's saved from training script)\n",
    "model_path = '../data/models/final_model' # Or path to a specific stage model\n",
    "normalize_path = '../data/models/vecnormalize.pkl'\n",
    "\n",
    "try:\n",
    "    model = PPO.load(model_path)\n",
    "    print(f"Model loaded from {model_path}")\n",
    "    \n",
    "    # To inspect model policy architecture\n",
    "    print("\nModel Policy Network:\n", model.policy)\n",
    "    \n",
    "    # You can also run a single evaluation episode directly here if needed\n",
    "    # For comprehensive evaluation, use the 'eval' mode of training.py\n",
    "    # env = BalatroEnv()\n",
    "    # if os.path.exists(normalize_path):\n",
    "    #    from stable_baselines3.common.vec_env import VecNormalize\n",
    "    #    env = Monitor(env, 'temp_log')\n",
    "    #    env = make_vec_env(lambda: env, n_envs=1)\n",
    "    #    env = VecNormalize.load(normalize_path, env)\n",
    "    #    env.training = False\n",
    "    #    env.norm_reward = False\n",
    "    # obs, _ = env.reset()\n",
    "    # for _ in range(100):\n",
    "    #    action, _states = model.predict(obs, deterministic=True)\n",
    "    #    obs, reward, done, truncated, info = env.step(action)\n",
    "    #    env.render()\n",
    "    #    if done or truncated:\n",
    "    #        break\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f"Error loading model: {e}. Ensure the model has been trained and saved correctly.")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section would involve deeper analysis of agent behavior.\n",
    "# E.g., analyzing decision points like when to discard vs. play, which hands are preferred.\n",
    "# This requires saving more granular data during evaluation or logging specific actions.\n",
    "\n",
    "# Example: Plotting hand type distribution from BalatroMetrics\n",
    "if metrics_obj.hand_types_played:\n",
    "    print("\n--- Hand Type Play Frequency ---")\n",
    "    print(metrics_obj._get_most_common(metrics_obj.hand_types_played))\n",
    "\n",
    "# You could extend BalatroMetrics or add functions to `utils.py` to log and analyze:\n",
    "# - Number of discards per blind\n",
    "# - Money spent in shop (if shop is implemented in env)\n",
    "# - Value of cards discarded vs. played\n",
    "# - Correlation between joker types and performance\n",
    "\n",
    "print("\nStrategy analysis requires more detailed logging within the environment or custom callbacks.")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
